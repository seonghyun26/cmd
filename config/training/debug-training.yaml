batch_size: 4096
shuffle: True
epoch: 8
loss: MSE
loss_variance: False
optimizer:
  name: Adam
  params: {
    lr: 1e-3
  }
# scheduler:
#   name: ExponentialLR
#   params: {
#     gamma: 0.99
#   }
# scheduler:
#   name: ReduceLROnPlateau
#   params: {
#     gamma: 0.8,
#     milestones: [30, 80, 150]
#   }
scheduler:
  name: CosineAnnealingLR
  params: {
    T_max: 200,
    eta_min: 1e-5
  }
num_workers: 8
train: True
eval_freq: 2
max_grad_norm: 10